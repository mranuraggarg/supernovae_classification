**Supernovae Type Ia Classification**
A machine-learning pipeline for  **Type Ia Supernovae classification**, leveraging  **XGBoost, Random Forest, and Linear Models**. This repository is inspired by  [Adam Mossâ€™s Supernovae Dataset](https://github.com/adammoss/supernovae)  and extends it with optimized training and evaluation techniques.

**ðŸ“Œ Project Overview**
This project provides an end-to-end pipeline for classifying Type Ia Supernovae using multiple machine-learning models. The workflow consists of three key stages:
1.  **Preprocessing**  - Converts raw data into a structured format.
2.  **Model Training**  - Trains six models with optimized hyperparameters.
3.  **Evaluation**  - Runs trained models on the test dataset and presents final results.

The repository allows users to either  **run pre-trained models**  or  **train models from scratch**, depending on their requirements.

----------

**ðŸ› ï¸ Installation and Setup**

This project uses  **Conda**  for environment management to ensure reproducibility.

**1ï¸âƒ£ Create and Activate Conda Environment**

```bash
conda env create -f environment.yml
conda activate ds_new  # Ensure the environment name matches  
```

**2ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/your-repo/supernovae-classification.git
cd supernovae-classification
```
**ðŸš€ Usage Guide**

  

This repository supports three different execution modes based on user needs.

  

**1ï¸âƒ£ Running Pre-Trained Models**

  

If you only want to evaluate pre-trained models and skip training, run:
```bash
python main.py
```

â€¢ **What happens?**

â€¢  Loads the existing models from  models/{model_name}

â€¢  Uses the preprocessed dataset  supernovae_dataset.npz

â€¢  Evaluates all six models and presents results in a structured format

  

âœ… **No training or preprocessing is performed** in this mode.

----------

**2ï¸âƒ£ Training All Models**

  

If you wish to  **train all six models**  from scratch using preprocessed data:

```bash
python train.py
```
â€¢ **What happens?**

â€¢  Uses the dataset stored in  supernovae_dataset.npz

â€¢  Trains six models sequentially:

â€¢ **Linear Models (with and without SMOTE)**

â€¢ **XGBoost (with and without SMOTE)**

â€¢ **Random Forest (with and without SMOTE)**

â€¢  Saves trained models in  models/{model_name}

â€¢  Generates a results summary

  

âš ï¸  **Note:**  Training all models is computationally expensive and was originally performed using:

â€¢ **Colab A100 GPU** for **XGBoost training**

â€¢ **Apple M1 GPU** for **Linear Model training**

  

ðŸš€  **Recommended:**  Run this step on a machine with a  **powerful GPU**.

----------

**3ï¸âƒ£ Full Pipeline Execution**

  

To  **start from raw data**  and execute the entire workflow:

```bash
tar -xvf SIMGEN_PUBLIC_DES.tar.gz && python preprocess.py && python train.py && python main.py
```
â€¢ **What happens?**

1. **Extracts raw supernovae dataset** (from SIMGEN_PUBLIC_DES.tar.gz)

2. **Runs preprocessing** (preprocess.py) to generate supernovae_dataset.npz

3. **Trains all six models** (train.py) and saves them in models/{model_name}

4.  **Evaluates trained models**  (main.py) and presents a final summary

  

ðŸ“Š  **Final Output:**  A structured  **Pandas DataFrame**  comparing all trained models.

----------

**ðŸ“‚ Repository Structure**

```
supernovae-classification/
â”‚â”€â”€ data/                        # Raw and processed data
â”‚   â”œâ”€â”€ SIMGEN_PUBLIC_DES.tar.gz  # Raw dataset (original)
â”‚   â”œâ”€â”€ supernovae_dataset.npz    # Preprocessed dataset
â”‚
â”‚â”€â”€ models/                      # Trained models directory
â”‚   â”œâ”€â”€ linear/
â”‚   â”œâ”€â”€ xgboost/
â”‚   â”œâ”€â”€ random_forest/
â”‚
â”‚â”€â”€ scripts/                      # Python scripts for different tasks
â”‚   â”œâ”€â”€ preprocess.py             # Preprocesses raw data
â”‚   â”œâ”€â”€ train.py                  # Trains all models
â”‚   â”œâ”€â”€ main.py                   # Runs pre-trained models for evaluation
â”‚
â”‚â”€â”€ environment.yml               # Conda environment setup
â”‚â”€â”€ .gitignore                    # Files to ignore in Git
â”‚â”€â”€ README.md                     # Project documentation
```
----------

**ðŸ“ˆ Models Implemented**
|**Model Name**|**Data Balancing**  |**Algorithm Used** |
|--|--| --|
| Linear (Without SMOTE) |No  | MLP |
|Linear (With SMOTE)|Yes| MLP |
| XGBoost (Without SMOTE) | No | Gradient Boosting Trees |
|XGBoost (With SMOTE)|Yes| Gradient Boosting Trees |
|Random Forest (Without SMOTE)  | No |Random Forest |
|Random Forest (With SMOTE)|Yes| Random Forest|

  

We train  **six different models**  to compare their performance.

SMOTE (**Synthetic Minority Over-sampling Technique**) is used for balancing datasets where necessary.

----------

**ðŸ“Š Performance Summary**
| **Model** | **Precision** | **Recall**| **F1-Score** | **ROC-AUC** |
|--|--| --|--|--|--|
| Linear (Without SMOTE) | 0.7494 | 0.7713 | 0.7553 | 0.7422 | 
|Linear (With SMOTE)| 0.7715 | 0.7881 | 0.7796 | 0.7611 | 
| XGBoost (Without SMOTE) | 0.9215 | 0.9224 | 0.9218 | 0.9738 | 
|XGBoost (With SMOTE)| 0.9197 | 0.9207 | 0.9200 | 0.9749 | 
|Random Forest (Without SMOTE)  | 0.9098  | 0.9118 | 0.9098 | 0.9597 | 
|Random Forest (With SMOTE)| **0.9268** | 0.9235 | **0.9246** | 0.9683 | 
|XGBoost (Without SMOTE and optimized)| 0.9230 | **0.9240** | 0.9234 | **0.9760** |

ðŸ›   **Final selection:**  **XGBoost (Without SMOTE)**  was found to be the most optimal model based on  **ROC-AUC > 0.97**.

----------

**ðŸ“š Acknowledgments**

â€¢  **Data Source:**  The dataset and preprocessing pipeline are borrowed from  [Adam Mossâ€™s Supernovae Dataset](https://github.com/adammoss/supernovae).

â€¢  **Inspiration:**  Inspired by existing works in Type Ia Supernovae classification.

----------

**ðŸ“œ License**

  

This project is licensed under the  **MIT License**. Feel free to modify and use it as per your needs.

----------

**ðŸ’¡ Future Work**
â€¢  **Hyperparameter Optimization**: Further fine-tuning XGBoost with Bayesian Optimization.

â€¢  **Ensemble Learning**: Combining multiple models to improve accuracy.

â€¢  **Deep Learning**: Exploring Transformer-based approaches for classification.

----------
